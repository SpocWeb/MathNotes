We're continuing our discussion of spinors in 3D from a mathematical point of view. In the last couple videos, we've been using Sigma matrices as basisv ectors in the XYZ directions. This might lead you to ask what the sigma matrices actually are. Should we view them as basis vectors, like little arrows in 3D space in the XYZ directions? Or should we view them as matrices? One answer to this question involves Clifford Algebras which, I'll talk about in a later video. In this video I'm going to try explaining the relationship between vectors and Pauli spinors by showing that the sigma matrices form a link between ordinary 3D space and a pair of two-dimensional spinor spaces. I'm going to show that the sigma matrices actually have three: indices one vector index (also called a tensor index) and a pair of spinor indices. The sigma matrices act as a link between tensor indices and spinor indices, allowing us to replace one tensor index with a pair of spinor indices. This means that you can use the sigmas to rewrite any tensor you've seen using spinor indices, which doubles the number of indices. This is related to the concept of the Infeld-Van der Waerden symbols. Although the Infeld-Van der Waerden symbols are only used for space-time, and I will cover them more fully in the next video. To help explain this, we'll see that a Pauli Vector can be written as the tensor product of a spinor and a dual spinor. Explaining all of this will require some basic understanding of tensor concepts as a prerequisite: like the tensor product and dual vectors (also called co-vectors). I'll go over a quick review of these concepts in this video, but I've included links in the description to my "Tensors for Beginners" series if you want a more in-depth explanation. Before we start, I want to correct a couple things from the previous video. When factoring a zero-determinant Pauli Vector, I pulled this "-1" outside the square root and wrote it as "i". This is a bit problematic because the square root of "-1" can be either "+i" or "-i". This leads to different sign conventions for the results, although this is expected since we already know there are multiple solutions for the spinor components. The most common convention in textbooks is to leave the negative sign under the square root and get these formulas for the spinor components. Also, in previous videos I've been saying "Paw-lee" instead of Pauli ("Pow-lee"). I still say it wrong in some parts of this video but moving forward I'll try to use the correct pronunciation. Let's start by reviewing some basic tensor concepts. If I have a vector "v" living in two-dimensional space, I can write it as a linear combination of basis vectors e1 and e2 using components v1 and v2. Notice that I've written the indices on v1 and v2 as upper indices--these are NOT exponents. One reason for this is that basis vectors are often written in a row, and so the lower index tells us the position in the row. And vector components are often written in a column, and the upper indices tell us their position in the column. Another reason I've written the vector components with upper indices is because vector components are contravariant. "Contra-variant" just means "opposite-change", meaning that vector components change in the opposite way that basis vectors do. For example, if this vector's components in this basis are [1,1], if I double the length of the basis vectors by multiplying them by 2, then I must halve the length of the components by multiplying them by 1/2, to get the components [1/2, 1/2]. The vector components changed in the opposite way that the basis vectors did. The basis vectors grew and the vector components shrunk. More generally, if I change basis vectors with a matrix, then I must change the vector components using the inverse matrix to balance the change out. I can also write this in summation notation like this. Some sources also choose to write these summation formulas without including summation symbols. You can tell a summation is happening whenever an upper index letter is matched with a lower index letter. This is called "Einstein summation notation". Next I'll introduce dual vectors, which go by many other names, including "covectors" and "one-forms". The bra-vectors from quantum mechanics are just another name for dual vectors. Ordinary vectors are like arrows and are good for representing objects with a length and a direction, such as velocity. Dual vectors are like oriented stacks and are good for representing objects with a density and a direction, such as a traveling wave. Larger dual vectors correspond to denser stacks and smaller dual vectors correspond to less dense stacks. A dual vector is a machine that takes a vector and outputs a scalar just by counting the number of stack sheets that are pierced by the vector, which is 3 in this example. This idea of counting the number of stack sheets that a vector pierces gives these two linearity laws for dual vectors acting on vectors. The components of a dual vector "Alpha" are given by counting how many stack sheets are pierced by the basis vectors. Here, the components of alpha are [3,2]. As I said, vector components normally go in columns, with the upper index indicating their position in the column. Dual vector components normally go in rows, with their lower index indicating their position in the row. Dual vector components are written with lower indices because they are "covariant". This means they change in the same way as basis vectors. Let's say we have this dual vector with components [2,4]. If we double the length of the basis vectors. then they will pierce twice as many stack sheets, and so the dual Vector components also double to [4,8]. We can also define the dual basis, written with epsilons. These are defined by this equation: if the dual vector index matches the vector index, we get 1. And if they don't match, we get 0. For example epsilon^1 acting on e_1 gives 1, and the same for epsilon^2 acting on e_2. And if the indices don't match we get 0. So epsilon^1 acting on e_2 gives 0, and same with epsilon^2 acting on e_1. Dual basis vectors are contravariant. If we double the length of the basis vectors, we must halve the length of the dual vectors to make sure the equation stays true. As I said, vector components normally go in columns, with the upper index indicating their position in the column. Dual basis vectors with their upper indices also go in columns. When we have vectors living in a vector space V, we say that the Dual vectors live in the corresponding dual Vector space. This is often denoted by V* in many books, but I'm just going to write the dual space as V-dual. We can do row-column multiplications to get the scalar result of a covector acting on a vector. This is normally called the "inner product". But we can also do column-row multiplications to give us a 2x2 matrix, which functions as a linear map. This is normally called the "outer product". This outer product array multiplication can also be written abstractly with symbols using this tensor product symbol. If we multiply this array product by a scalar, we can distribute the scalar to either the column or the row. The tensor product also obeys this rule, where we can distribute the scalar either to the left entry or to the right entry. Also if we have a column multiplied over a sum of rows, we can distribute over the rows. Similarly if we have a sum of columns multiplied by a row, we can distribute the row over the columns. The tensor product follows similar rules, where if we have a sum on the right side, we can distribute over it. The same goes for distributing over a sum on the left side of the tensor product. Some matrices, like this one, can be written as the product of a single column times a single row. Or in abstract notation, the tensor product of a single vector and a single dual vector. However, other matrices like this one, whose determinants are non-zero, cannot be written as a single column times a single row. Instead, we break these apart into a sum of matrices, one for each component, bringing the scaling factors outside. Each of these matrices now has a determinant of zero, so we can factor them into a column and a row. This shows that we can write any Matrix as a sum of column-row products, scaled by the appropriate coefficients. The same is true for tensor products. Any linear map can be written as a linear combination of tensor products of basis vectors and basis dual vectors, all scaled by the appropriate coefficients. I should also mention that in quantum mechanics, vectors are often written with the KET notation, and dual vectors are often written with the BRA notation. Ihe inner product is then written as a BRA-KET, and the outer product is written as a KET-BRA, which can include or omit the tensor product symbol depending on the book. All of this reasoning that we've used for vectors can also be used for spinors. We have spinors which live in a spinor space S, and dual spinors which live in the dual spinor space S-dual. I'm going to denote spinors by bold letters. Let's say that we have a spinor psi that lives in S. A general spinor can be written as a linear combination of basis spinors with lower indices, and spinor components with upper indices. We can also write this in column-row notation like this, or in summation notation like this. Similarly a dual spinor Zeta that lives in S-dual can be written as a linear combination of dual basis spinors with upper indices, and dual spinor components with lower indices. Again we can write this in column-row notation and summation notation. As you would expect, basis spinors are covariant and spinor components are contravariant. Dual spinor components are covariant and dual basis spinors are contravariant. We can also do inner products of spinors, which gives a scalar... And outer products of spinors. So, so far, it doesn't seem like there's much of a difference between vectors and spinors. They both live in what is formally called a "vector space". They both obey the same rules. The difference between them lies in their relationship with each other. In the last video, we learned that we can take a Pauli vector with a determinant of zero and write it as the product of a column spinor and a row spinor. And I'm going to rewrite the components of the row spinor using Zeta with lower indices, treating it as a dual spinor. On an abstract level, what we're doing here is writing a Pauli vector as the tensor product of a spinor and a dual spinor. More generally, if a Pauli Vector has a non-zero determinant, we can break it up into four matrices, each with zero determinant, then write each of those as a column spinor times a row spinor. More abstractly, we write this as a sum of tensor products between basis spinors and dual basis spinors, like this. We can also write these outer products in bra-ket notation if we want. Looking at the V components, the upper index specifies the position in the left column, or which row in the matrix. And the lower index specifies the position in the right row, or which column of the matrix. Each of the sigma matrices can be written this way as well. For example we can write sigma_x as a linear combination of matrices, broken into column-row products. More abstractly this is a linear combination of tensor products of spinors and dual spinors. The same can be done for sigma_y and sigma_z. Looking at these three formulas, we have 12 coefficients that tell us how to sum all these tensor products together. So now let's answer the question: what actually are the sigmas? Well, Sigma is actually a linear map from the ordinary 3D Vector space V to the four-dimensional space of spinor-dual spinor pairs, which is S-tensor-S-Dual. If we take our e_x basis vector from ordinary 3D space and act on it with sigma, we get a linear combination of the four basis elements of the space S-tensor-S-Dual. The coefficients are 0,1,1,0, which are exactly the entries of the matrix sigma_x. We can do the same thing with e_y and get the coefficients 0,+i,-i,0, which are the entries of sigma_y. And with e_z we get the coefficients 1,0,0,-1, which are the entries of sigma_z. We can label these 12 coefficients using Sigma with three indices to explain why these indices make sense let's review an ordinary linear map L from 3D space with one basis mapping to another 3D space with another basis, with tilde's (~) on top. If we act on e_1 with L the output Vector is some linear combination of the three tilde (~) basis vectors. The coefficients are written L with a 1 as the lower index because the input Vector was the e_1 basis Vector with a lower index. The upper indices are written 1,2,3 to match with the indices of the three e~ basis vectors. We can get similar sets of coefficients for L(e_2) and L(e_2). Notice that these raised indices of the coefficients are opposite the lowered indices for the basis vectors they are associated with this is because the upper coefficient index and the lower basis vector index are summed together and so they must have opposite elevations. Now let's look at the linear map Sigma acting on a basis Vector e_x. The output is a linear combination of the four spinor-dual spinor products. Each of the coefficients are written as sigmas with a lower "x" index to match the lower "x" index in the e_x basis vector. We then require a pair of spinor indices to tell us which basis element the coefficients are associated with labeled with: (1,1) (2,1) (1,2) and (2,2). Note that the component indices are written with opposite elevations compared to the basis elements. For example, the basis element with a lower 2 and upper 1 is matched with a coefficient with an upper 2 and a lower 1. We can do something similar for e_y and e_z. And we can summarize all three of these formulas using Einstein's summation notation. So when we map a general 3D Vector small v through the linear map Sigma, we can apply the linear map to each basis vector individually, with the components brought out in front. We get a linear combination in our 4-dimensional space of spinor-dual spinor products. And these are the four entries of the corresponding Pauli Vector big V. We can also write this as an array multiplication, or as this summation. So the linear map Sigma forms a link between our ordinary 3D Vector space V and the 4D space of spinor-dual spinor products. When we wrote a 3D Vector small v in the form of a Pauli Vector big V, we were really putting the small V vector through the linear map Sigma and writing it in the space of spinor0dual spinor tensor products. This is what allows us to factor certain Pauli vectors into a spinor and a dual spinor. Note that all Pauli vectors can be represented in our spinor-pair space, but not every element of our spinor-pair space corresponds to a Pauli vector. Pauli vectors must be traceless and hermitian matrices, and our 4D spinor-pair space can make any general 2x2 matrix with complex entries. Also note that this map turns a single Vector index into two spinor indices. So in some sense, one vector index is worth two spinor indices. We can apply this Sigma map to any tensor T that we like, and replace every tensor index with a pair of spinor indices. This is why spinors can be thought of as a tensor of rank 1/2. A vector, which is a rank 1 tensor, is equivalent to the product of a pair of spinors, each of rank 1/2. And a single Vector index requires two spinor indices to be represented. And, as seen in previous videos, when physical space is rotated by an angle "theta", the pair of spinors in the corresponding spinor spaces each rotate with half that angle: "theta/2". The last thing I'll mention is that, just as it's possible to change basis in our ordinary 3D Vector space V, it's also possible to change basis in our spinor space S. For example, we can double the length of our s_1 basis spinor. Since dual basis spinors are contravariant, the dual basis spinor s^1 must transform in the opposite way with multiplication by 1/2. So our change of basis equations for spinors and dual spinors are here. And we can rearrange these so that the tilde basis is on the right side of the equals sign. If we take a generic element of our spinor pair space written in the original basis, we can change to the tilde basis by applying these equations one-by-one to each term in the sum. The 1,1 term gets a factor of 1/2 and 2 that cancel. The 2,1 term gets a factor of 2. The one 2,1 term gets a factor of 1/2. And the 2,2 term gets no additional factors. This gives the same object but written in the tilde (~) basis instead. You can see here that there are some extra numerical factors in front of some components. We can also write the new components as V~. So both of these formulas are just different ways of writing the same output from the sigma map. But because they are written in different bases, they give us different components, and they correspond to different matrices. Again the components in the new basis can be written with V~. In this new spinor basis, the components of the sigma matrices would now contain these factors, meaning the sigma matrices would now look like this. Now, we won't be changing spinor basis very often in this video series, but just keep in mind it's something that can be done. And it will affect the components of the sigma matrices. So to sum up this video, just as vectors live in Vector spaces and dual vectors live in dual Vector spaces, we have spinors that live in spinor spaces and dual spinors that live in dual spinor spaces. Spinor spaces obey all the rules of linear algebra that would be expected from Vector spaces, but there is a special linear map Sigma that maps vectors from 3D space into the space of spinor-dual spinor tensor products. This is where Pauli vectors and the sigma matrices live, and it explains why we can Factor certain Pauli vectors into a spinor and a dual spinor. The linear map Sigma has 12 coefficients and it converts one tensor index into a pair of spinor indices. In the next video, when we cover spinors in four-dimensional space time, the equivalent Sigma map will be a 4x4 Matrix and its components are called the Infeld-Van der Waerden symbols.