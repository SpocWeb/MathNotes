We're continuing our discussion of Lie groups and Lie algebras. In the previous couple videos, we found the Lie group matrices for rotations of spin-0, spin-1/2, and spin-1 particles. And in this video we're going to come up with a general procedure for finding the Lie group matrices that rotate higher-spin particles, such as spin-3/2, spin-2, and all higher-spin particles. This procedure will involve the ladder operators we're going to see that the SO(3) group only has representations for integer spin particles if we want to find representations for half integer spin particles as well we need to look at its double cover su(2) before I talk about the procedure with the ladder operators. I'm going to be more formal about some definitions in representation theory because I've been sort of glossing over them in previous videos. So, I've already mentioned that a group is a set with an operation that obeys these four properties: closure, associativity, existence of an identity element, and existence of inverses. Lie groups have the additional property of being continuous. So what is the formal definition of a representation of a group? A representation is a function rho that takes every member in a group G and assigns it to an invertible n-by-n matrix. So rho is a function from the group G to GL(n), the general linear group of invertible n-by-n matrices. In order to be a representation, this function rho must preserve the group operation for all members in the group. This means that combining two group elements and then getting its representation matrix is the same thing as getting the representation matrices for both group members and then multiplying the matrices. A consequence of this is that the group's identity element must be mapped to the n-by-n identity matrix, and group inverse elements must be mapped to the corresponding representation matrix's inverse. Now, with Lie groups like SU(2) and SO(3), the groups are defined in terms of matrices to begin with. For example, SU(2) is defined as the set of 2x2 complex unitary matrices with determinant 1. We call these matrix definitions the "defining representation" of the group. But we can find matrices of other sizes that satisfy the conditions of a representation. For example, it's possible to map all SU(2) matrices to a set of 4x4 matrices which obey all the same matrix multiplication rules. It's also possible to map pairreps of SU(2) matrices each to an SO(3) matrix. So the group SO(3) is a 3x3 representation of SU(2). Some of the group structure is forgotten here because it's an irreversible 2-ti-1 mapping. The 1x1 representation of SU(2) sends all matrices to the 1x1 identity matrix. Here, all the group structure is forgotten, but it still technically preserves all the multiplication rules. We can also find representations for Lie algebras, which is a function pi that sends Lie algebra members to n-by-n matrices. Here the function pi must preserve the Lie bracket of the algebra. Now, I mentioned we can find a 4x4 representation of the Lie group SU(2), but we've never seen this before. How would we go about finding this 4x4 representation? Well there's one way of getting a 4x4 representation that's sort of cheating. We could take a pair of 2x2 SU(2) matrices and take their direct sum, which means arranging them in block diagonal form so that the first one transforms the top two components of a column and the second one transforms the bottom two components of a column. When multiplying block diagonal matrices like this, the upper blocks will only multiply with upper blocks and the lower blocks will only multiply with lower blocks. So it's like we've put two independent copies of SU(2) into the same matrix. This would satisfy the requirements of a 4x4 representation. Alternatively, we could take a 3x3 representation and the 1x1 representation and arrange them in Block diagonal form. When we construct a representation using smaller representations with the direct sum to give a matrix in block diagonal form, we call this representation "reducible". If a representation cannot be written as a direct sum, then we call it an "irreducible representation" or "irrep" for short. In this video, we're interested in finding irreducible representations or irreps, so we're ignoring the case of building representations from smaller representations using the direct sum. We'll see later that for the SU(2) group, there is exactly one unique representation for a given matrix size. We can label these representations by the matrix size "n". also called the "dimension". We can also label these representations by the spin value, which is the (dimension-1)/2. We're also going to prove that the SO(3) group only has odd-dimensional representations. Or equivalently, only integer-spin representations. So before we learn the procedure for building general representations of su(2), we're going to spend some time studying the 2x2, or spin-1/2, representation. We'll start with the spin-1/2 representation of the su(2) Lie algebra, then we'll define the ladder operators. And from there, we'll generalize the ladder operators to any spin, and use them to build all higher-dimensional representations. We've seen that one way to get Lie group matrices is to take a basis of generators from the group's Lie algebra and exponentiate them. We know the three 2x2 generators for su(2) are given by -1/2 * pairs of sigma matrices. These give a basis for 2x2 traceless anti-hermitian matrices. I'm now going to introduce the raising and lowering operators g+ and g-, which are also called ladder operators. Recall that the spin-up spinor state is represented by the column [1,0]. And the spin-down spinor state is represented by the column [0,1]. We'll define the raising operator as changing the down state into the up state, and changing the up state into the zero spinor. We'll define the lowering operator as changing the up State into the down state, and changing the down state into the zero spinor. The raising operator g+ is given by the matrix [0 1; 0 0]. And the lowering operator g- is given by The matrix [0 0; 1 0]. You can see they obey the expected properties. Given the su(2) generators, to build g+, we can split it into symmetric and anti-symmetric parts. Then if we pull out a factor of "i", we can can see that we can rewrite g+ as i*gyz - gzx. We can do something similar with g- to write it as i*gyz + gzx. I'm going to define one last operator gz. When this acts on the up state it returns +1 times the up state. When it acts on the down state, it returns -1/2 times the down state. In other words, the up and down states are eigenvectors of the gz operator with eigenvalues +1/2 and -1/2. In this basis, the gz matrix would be diagonal with entries +1/2 and -1/2. This matrix is simply equal to i*gxy. Also, it's very common to label the up and down States after their eigenvalues under gz. So the up state could be labeled with +1/2 and the down state could be labeled with -1/2. This is related to why we call the 2x2 representation the spin-1/2 representation. The reason I'm calling this matrix "gz" is because this is the matrix physicists use when measuring the spin angular momentum of a particle along the z-axis, or equivalently the xy plane. It's also worth noting that since gxy is anti-hermitian, then gz must be hermitian. This implies all the eigenvectors of gz are orthogonal, so the eigenvectors of gz give us an orthogonal basis. So, using our original su(2) basis, we've defined three new operators: the raising operator g+, the lowering operator g-, and the eigenvalue operator gz. Note that since we define these raising and lowering operators using complex coefficients, this means we are leaving the su(2) Lie algebra and going to its complexified version. Also, since most physics textbooks use hermitian matrices for the su(2) generators, you might find the definition of these operators look a little different, but they behave the same way. Now, let's look at the commutators of these three matrices. To help compute these, I'm going to look at some properties of the commutator. The commutator of matrices X and Y is XY-YX. Notice that if we swap the order of the commutator inputs, we get a negative sign compared to the original commutator. This also means the commutator of any matrix with itself is always zero, since the two terms cancel. Now, let's look at linearity properties. Let's say we have matrices X,Y,Z, and scaling numbers a and b, and we want to determine the commutator of Z and "aX+bY". We can write out the commutator as the subtraction of the two products. After distributing, we can factor out a from the Z and X terms, and factor b from the Z and Y terms. These then become the [Z,X] and [Z,Y] commutators. So what we've learned here is that the second input in the commutator bracket is linear. We can either add and scale matrices inside the commutator input, OR add and scale the commutators of the individual matrices and get the same result. The first input of the commutator also obeys the linearity property, so we say the commutator is "bilinear". I'm now going to compute the the commutator of g+ and g- you can pause now if you want to try it yourself. First we sub in their definitions, and then use linearity rules once, and then twice to get four terms. The commutator of a term with itself is always zero, so the first and fourth terms go to zero. We can put the factors of "i" out in front and swap the order of the inputs. In the first remaining commutator, if we change its sign, we end up with 2i*the commutator of gyz and gzx, which we know is gxy. And using the definition of gz, this gives us 2gz. You can also pause and try to calculate the commutator of gz and g+, with the answer being g+. And calculating the commutator of gz and g-, the answer ends up being negative g-. So we have this new basis of the complexified Lie algebra su(2), and this gives us a new set of commutators. So what was the point of this change of basis? Well, we know gz acting on the +1/2 state gives eigenvalue +1/2, and gz acting on the -1/2 state gives an eigenvalue of -1/2. We can summarize this by saying gz acting on the m State gives an eigenvalue of m. But what happens if we have gz acting on the state g+|m>? We can take the product gz * g+ and see it in this commutation relation. This means we can replace it with the commutator of gz and g+ added with g+ * gz. We know from previous slides that this commutator just gives g+, and when gz acts on the m state, it gives the eigenvalue m. We can then factor out g+ from these terms and get m+1 out in front. What this means is that we can treat g+|m> as a new eigenvector of the gz operator, and it has eigenvalue m+1. This makes sense, given what we expect from the raising operator. It should raise us from the -1/2 state to the +1/2 state, thereby increasing the eigenvalue by 1. Now recall that the g+ operator will send the vector with the highest eigenvalue to zero. This is true in the 2x2 representation, but it's true in any finite dimensional representation, because we'll eventually run out of basis vectors to raise at some point. We call the basis vector that g+ sends to zero "the vector with the highest eigenvalue". We sometimes call eigenvalues "weights" in this context, so this is also called the "eigenvector with the highest weight". We can do something similar with the lowering operator g-, except this time the commutator gives negative g-. And so we end up with an eigenvalue of m-1. Again, this makes sense. We expect the lowering operator to decrease the eigenvalue of a state by 1, except in the case of the lowest eigenvalue or lowest weight vector, which gets sent to zero. It turns out that if the highest eigenvalue is j, then the lowest eigenvalue will be -j. I'll prove this a bit later in the video. So let's think carefully about what we've just seen with these eigenvalue relations, thinking in terms of any dimension, not just the 2x2 representation. When we applied gz to the state g+|m>, we got an eigenvalue of m+1. So this means that the g+|m> state behaves like the state |m+1>. However, since the state appears on both sides of the equation, we have only showed that g+|m> is proportional to the state |m+1>. They are not necessarily equal. They're only equal up to some scaling factor alpha. Similarly with the g-. The state g-|m> behaves like the state |m-1>, and is proportional according to some proportionality constant beta. Now, to solve for alpha and beta, I'm going to make the assumptions that the igenvectors of gz that we're working with are all normalized, so the bra-ket inner product of any gz eigenvector with itself always equals 1. To get the norm of g+|m>, we use its corresponding bra-vector, which is its hermitian conjugate. This equals the bra version of m, let's take their inner product. We know g+ raises the state to |m+1> with a proportionality constant alpha for the ket vector. g+(dagger) should do the same thing on the bra vector, but it gives the proportionality constant of alpha-complex-conjugate instead. This is because the hermitian conjugate of a scalar is just the complex conjugate. Putting the scalers out in front, alpha-conjugate times alpha is just the squared magnitude of alpha. And since these state vectors are normalized, we just get 1. So we've shown that the squared norm of the g+|m> state is the magnitude of alpha squared. We can similarly show that the squared norm of the g-|m> state is the magnitude of beta squared. So let's calculate g+(dagger)*g+ by plugging in its definition. We can pass the dagger to all the terms inside the brackets, which also flips the sign of the complex "i". if we distribute the four terms, we can write -i*i as 1. Now, we know all the su(2) generators are anti-hermitian, so the dagger operation just gives a minus sign. With what's left, we get gyz^2 + gzx^2 - i*(this commutator), which we know is just gxy. And we know i*gxy is the definition of our eigenvalue operator gz. We can do a similar calculation for g-(dagger)g-. The only difference is that gz has a plus sign instead of a minus sign. So we have these two formulas. To understand these formulas better, I'm going to talk about the Casimir operator g^2, which is defined as the negative squares of the su(2) generators summed together. g^2 is important because it commutes with everything in the Lie algebra. This g^2 operator is NOT a member of the Lie algebra su(2) because the Lie algebra doesn't understand what squaring matrices means. Lie algebras only understand the Lie bracket (or commutator). The Casimir operator g^2 instead lives in the universal enveloping algebra, but I'm not going to talk about that in this video. To prove that g^2 commutes with everything, I'm going to go over a couple more properties of the commutator. First, the commutator of A with A^2 always goes to zero, because the two terms cancel. Also, the commutator obeys a kind of product rule. If we take the commutator of A with B*C, we can subtract and add BAC in the middle and get a sum of two commutators, one with B and one with C. Using this, we can prove gxy commutes with g^2. We can write out the definition of g^2 and then distribute. We know the commutator involving only gxy goes to zero automatically. The other two terms can be evaluated using product rule, giving two terms each. We can rewrite these commutators as gzx and gyz. Working out the signs, all the terms cancel and we see the commutator goes to zero. We can use similar proofs to show that g^2 commutes with the other two basis vectors, and so commutes with every element of su(2). So knowing the defition definition of g^2 and also the gz operator, we can move gxy^ 2 over to the other side and rewrite it in terms of gz. Finally, we can take the g+ and g- formulas from before and rewrite the gyz and gzx parts using g^2 and gz instead. So now these products are written entirely in terms of g^2 and gz. So what does this Casimir operator g^2 give when it acts on the vector |m>? Well, let's take m to be the highest weight vector labeled j. Remember, this is the vector that g+ sends to zero. So if we apply g^2 to the highest weight vector |j>, the g+ acting on the vector |j> will immediately go to zero. And gz will simply return the eigenvalue j. So gz^2 + gz just gives us j^2 + j, which can be rewritten as j*(j+1). So knowing this, let's use the lowering operator to see how g^2 acts on the j-1 vector. We can replace the j-1 vector with g- acting on the j vector, divided by beta. However, since g^2 commutes with all the other g's, we can flip their order and get it to act on the j vector, giving j*(j+1). We can then use the lowering operator to return the j state to the j-1 state. So because g^2 commutes with everything, it also returns j*(j+1). When acting on the j-1 vector. And by induction g^2 returns j*(j+1) on all other eigenvectors of gz. When a representation has highest weight j, we call it the spin-j representation. This is why the 2x2 representation is called the spin-1/2 representation. So we can finally compute what this g+(dagger) g+ gives when acting on the m vector. The g^2 part gives us j*(j+1) and the -gz^2 - gz part gives us -m^2-m, which can be rewritten as -m*(m+1). Plugging this into our formula for alpha, we can pull all the scalar parts out in front, send the norm of |m> to 1 since it's normalized, and take the square root of this formula to get alpha. Technically, alpha could be a complex number with a complex phase, but we can choose a phase convention where alpha is always a real number. We can do something similar for g-, where the only difference is a minus sign in the formula here. And this gives us a formula for beta. So finally, we now have the formulas for the alpha and beta coefficients for the raising and lowering operators. And these work in any dimension. I'm going to point out that in the alpha formula, if we plug in m=j, we get zero. This means that g+ cannot raise the j state. So j is the highest eigenvalue or highest weight. Similarly, for beta if we plug in m=-j, we get zero. So g- cannot lower the -j state. So -j is the lowest eigenvalue or lowest weight. This means that in a given representation, the basis vector weights range from +j to -j in increments of 1. This means that the highest weight j must be an integer or half-integer. Otherwise, increments of 1 would mean that the highest and lowest weights would have different magnitudes instead of equaling +j and -j, which is required. This also means that the dimension size n of the representation is always equal to 2j+1 for spin-j, since a highest weight of j implies there are 2j+1 basis eigenvectors. So we've covered a lot over the course of this video. We started with the three generators in the su(2) Lie algebra, then we changed basis to g+, g- and gz, and determined their commutation relations. We also found these eigenvalue relations for the operators and we saw g+ and g- raise and lower the eigenvector states according to the proportionality factors alpha and beta. And we determined these formulas for the various alpha and and beta coefficients, which depend on their representation spin number j and a vector's eigenvalue m. Now, in the case of the 2x2 representation, none of this gives us any new results that we didn't know before. The alpha and betas just end up being 1. But the important fact here is that the truth of these formulas does not depend on the dimension of the representation. We can use these formulas for the spin-1 case of 3x3 matrices, the spin-3/2 case of 4x4 matrices, the spin-2 case of 5x5 matrices, and so on. Let's try using all of this to build up the 3x3 spin-1 representation here. We'll have three basis state vectors, which I'm going to label as +1, 0, and -1. The highest weight here is +1, so this is the spin value j. Also, keep in mind this vector labeled with zero is NOT the zero vector. It's just the label we're giving to this particular basis vector with eigenvalue zero. For the spin-1 case, we know that j=1. So j*(j+1) = 2. Now, let's calculate these proportionality constants for the raising and lowering operators. For g+ acting on the -1 state, we sub in m=-1 and get a constant of the square root of 2. And here the state is raised to the zero-state. For g+ acting on the zero-state, we sub in m=0, and we get a constant of the square root of 2 next to the 1-state. We can also check that g+ acting on the 1-state goes to zero, but this is expected since there are only three basis vectors and we can't raise the highest weight vector to a higher one. We can also apply g- to the three states to get coefficients of the square roots of two, except in the -1 state case where we get zero since we can't lower it anymore. Recall, for a given matrix we can get one of its entries using the appropriate row vector on the left to select a row, and the appropriate column vector on the right to select a column. This outputs the scalar entry for that row and column. We can do the same thing for our 3x3 g+ matrix and get all nine of its entries using the various bras and kets on either side. Most of the entries go to zero, but a couple of them end up being the square root of two. These entries are used to raise the various state eigenvectors. We can do something similar with the g- operator to get this 3x3 matrix. And also the gz matrix, but that's pretty obvious since all the eigenvalues are already on the diagonal. Now that we have matrices for g+, g- and gz, we can use our change-of-basis equations to change back to the standard basis for generators of 3D rotations. These indeed give traceless, anti-hermitian matrices that we would expect from the su(2) Lie algebra. Now this is a different 3x3 solution than the one we've seen in previous videos, but we can switch between these new ones and the more familiar 3x3 solution just by using a change-of-coordinates. The two solutions are equivalent. Now, if we want to see the spin-1 representation of the SU(2) group, we just exponentiate the generator matrices in the Lie algebra to get these rotation matrices in the Lie group. Okay, so we saw that for the spin-1 representation of su(2), the procedure was to calculate the proportionality constants, then get g+, g- and gz as matrices, then switch back to the standard basis and get the su(2) Lie algebra generators. We can then exponentiate them to get the SU(2) Lie group matrices. We can use this same procedure again for the spin-3/2 representation of su(2), which uses 4x4 matrices. The four basis vectors would then be labeled with eigenvalues +3/2, +1/2, -1/2, and -3/2. The highest weight is 3/2, so this is the spin-3/2 representation. I'd encourage you to pause the video and try finding the spin-3/2 su(2) Lie algebra representation before I give the answer. Calculating the the proportionality constants is a bit more involved this time, but for gz we get the square root of 3, 2, and square root of 3. And for g- we get similar results. This gives us the following g+, g-, and gz matrices. Changing basis, we get these 4x4 generators. While exponentiating the gxy generator is easy, exponentiating the other generators is trickier so I'm not going to do it in this video. I'll also show you the 5x5 solution for spin-2. So pause now if you want to try. The g+ proportionality constants are 2, the square root of 6, the square root of 6, and 2. And same for g-. And we get these matrices for g+, g-, and gz. And we can change basis to get these generators. Exponentiating the gxy gener generator, we easily get a 5x5 SU(2) group matrix for doing rotations in the XY-plane. In the convention used in this video, the XY generators and group matrices are always diagonal. Let's take a look at how some particles rotate in the XY plane under the SU(2) group. Spin-0 particles transform with an identity matrix, so they don't change under rotations. For spin-1/2, particles we get two states: the spin-up and spin-down states, whose complex coefficients change in opposite directions under this rotation. It's also worth noting that the 1/2 coefficients in the exponents mean the period of rotation is two full turns, or 4pi. For spin-1 particles, we get three states: the +1 and -1 states correspond to two classical waves that are left and right polarized around the z-axis. And they take a full turn of 2pi to get back to their starting point. The zero-state corresponds to the longitudinal polarization along the z-axis, which does not change when we rotate around the z-axis, so it transforms with the constant 1. For massless spin-1 particles that travel at the speed of light, like photons, the longitudinal polarization is not possible so we only have the +1 and -1 states. For spin-3/2 particles, we get four states. The + and - 1/2 states have a period of TWO full turns, but the + and - 3/2 states have a period of 2/3 of a full turn. However overall, the states collectively have a period of two full turns or 4pi. or spin-2 particles, we get five state states. The zero-state is constant under these rotations. The +1 and -1 states have a period of one full turn. And the +2 and -2 states have a period of half a full turn. Collectively the states have a period of one full turn, or 2pi. For massless spin-2 particles that travel at the speed of light, like the hypothetical graviton, the inner three states are not possible, so we only have the +2 and -2 states. Classically, gravitational waves come in two polarizations: the plus and cross polarizations, where the amplitudes move in and out along the directions of plus and cross shapes. The +2 and -2 states correspond to specific linear combinations of the plus and cross polarizations. These polarizations come back to themselves after half a full turn, so their period is half of 2pi. Now, it can be confusing to tell the difference between the +1 state from the spin-1 representation and the +1 state from the spin-2 representation. For this reason, we often include the spin number j as a prefix label on the eigenstates. This helps us keep track of which eigenvector belongs to which representation. Now, it's a fact that there is exactly one irreducible representation of SU(2) per dimension. If you find two different irreps of the same size, you can always swap between them using a change of coordinates. This happens because every su(2) Lie algebra irrep allows us to define the ladder operators. And assuming a finite dimensional representation, there will always be a highest weight vector |j>. We can then use the lowering operator to create an orthogonal basis for this irrep. So if we have two su(2) Lie algebra irreps of the same dimension, pi and pi~, each with different generators, we can always use their respective ladder operators to define an orthogonal basis for each representation. We can then use a change of basis matrix C to swap between these two bases. This means we can also change bases for any linear map L, and so we can always swap between the two su(2) generators using the same change of coordinates matrix C. The group representations of SU(2) also use the same change of coordinate matrix C to change representations because of the properties of matrix exponentials. So we know that su(2) has exactly one irreducible representation per dimension. Now we'll prove that SO(3) has only one reducible representation for every odd dimension. This is a consequence of SU(2) being the double cover of SO(3). To prove this, we'll list a few facts. An important fact to remember is that a representation map rho will always send the identity element of a group to the n-by-n identity matrix. This is required to preserve the group multiplication rules. Now, the double cover map phi from su(2) to SO(3) sends the plus and minus versions of a given SU(2) matrix to the same SO(3) matrix R. As a result, the double cover map phi sends both the positive and negative 2x2 identity matrix to the 3x3 identity matrix. Now, if we have an irreducible representation rho of SO(3), if we combine it with the double-covering map phi, this automatically gives us an irreducible representation sigma of SU(2). Since phi sends the negative identity to the identity matrix, and rho sends the identity to the identity, this means the induced map sigma sends the negative identity in SU(2) to the identity matrix. Now let's look at the 4x4 and 5x5 irreps of SU(2). We know there's only one irrep per Dimension and we already know that this 2x2 diagonal matrix gets sent to this 4x4 and 5x5 diagonal matrix, respectively. Now, let's set theta to 2pi. All the exponents with a half-integer next to the angle will get sent to -1, since e^i*pi*(an odd number) is -1. And all the exponentials with integers next to the angle will get sent to +1 since e^i*2pi*(an integer) = +1. So in the 4x4 irrep, the negative identity gets sent to the negative identity. And in the 5x5 irrep, the negative identity gets sent to the positive identity. More generally, for every evendimensional or half-integer spin irrep, the negative identity is sent to the negative identity. But for odd-dimensional or integer-spin irreps, the negative identity is sent to the postive identity. Okay, let's bring this all together. We have the double cover map phi from SU(2) to SO(3), which sends the negative 2x2 identity to the 3x3 identity. And if we have an n-by-n irreducible representation of SO(3), this will send the 3x3 identity to the n-by-n identity, as required by the rules of representations. This also induces an n-by-n irreducible representation sigma of SU(2), which is the composition of phi and the SO(3) irrep. As a result, this sigma always maps the negative 2x2 identity to the n-by-n identity. Now let's include the information from the last slide. If the dimension n is odd, the n-by-n irrep of SU(2) will send the negative identity to the positive identity. This is consistent with what we have above so there are no problems. But if the dimension n is even, the n-by-n irrep of SU(2) sends the negative identity to the negative identity. This contradicts what we have above. The only conclusion we can draw is that there are no n-by-n irreducible representations of SO(3) for even dimensional n. If there was, sigma would always send the negative identity to the positive identity, which is not the case for even-dimensional irreps of SU(2). So we've proven that SO(3) only has irreps in odd dimensions, or equivalently, for integer spins. Now I'd like to move on to spacetime. But before I do, I want to point out that it's possible to Define representations for SU(2) using polinomial and derivative operators like this. These obey the expected commutation relations for SU(2). We can prove this using these identities and various commutator properties that we've seen before. I don't know if these have an official name, but we could call them "polynomial" or "differential" representations. In quantum mechanics, these are usually defined with i*hbar in front, and they generate rotation operations on wave functions. Now, we know that SU(2) has one irreducible representation per dimension. But we know from the last video that SL(2,C) in space time has two non-equivalent 2x2 irreducible representations, which we call the "left" and "right" representations. We cannot change between these representations using a change of coordinates. We know that with the su(2) Lie algebra, if we complexify it with complex coefficients, we can define the raising and lowering operators, which lets us define a representation's highest weight or spin value. With the sl(2,C) Lie algebra, if we complexify, it we can break it up into the direct sum of a pair of complexified su(2) Lie algebras, each with their own spin values. So a given sl(2,C) representation is defined by a pair of spin values instead of just one. We saw this trick of breaking up complexified sl(2,C) into a pair of complexified su(2)s in the last video. So let's see how this works. We start by taking the sl(2,C) rotation and boost generators, the J's and the K's, and change basis to the A's and B's. The only difference between the A's and B's is the sign in front of the K boost generators. These form two separate copies of the su(2) Lie algebra, which commute with each other. If we take each su(2) copy and complexify it so that they have complex coefficients, we can define raising and lowering operators and eigenvalue operators in each su(2) copy. This means we can label each su(2) representation with the highest weight or spin value. So this means that to label a given irreducible representation of sl(2,C), we need to pick two spin values. We refer to the first spin value as "left". And the second spin value, where the sign in front of the boosts is reversed, is called "right". These spin values are often written in an ordered pair like this. So for su(2) irreducible representations, we get a one-dimensional list of spin values. But for sl(2,C) irreducible representations, we get a two-dimensional grid which contains all possible pairs of left and right spin values. If we move from the Lie algebra sl(2,C) to the Lie group SL(2,C), the irreps are also labeled by a pair of spin values. This time the irreps can be written as the tensor product of a left representation and a right representation. We get a similar 2D table of irreps for the Lie group SL(2,C). This contains the representations which transform scalars, left and right Weyl spinors, as well as spacetime vectors. I'm not going to go into much more detail here because the next video will be all about the tensor products of representations. Dirac spinors transform with the direct sum of the left and right spin-1/2 representations so this is a reducible representation.
