So we’ve finished talking about basic examples of spinors in physics. Starting with this video, we’re going to start looking at spinors from a purely mathematical point of view, in both 3D space and 4D spacetime. In this video we’re going to see how we can re-write a 3D vector as a 2x2 matrix called a Pauli vector, and in the next video we’re going to see how we can factor this Pauli vector into a pair of Pauli spinors. Later we’ll see we can do the same thing for a 4D spacetime vector and factor it into a pair of Weyl spinors. For the next few videos, we’re going to be starting over and inventing spinors from scratch, so you won’t actually require any of the previous videos in to understand what’s going on. However the previous videos might help give more context for why we care about spinors. I’ve included a list of sources for this video in the description if you want to learn more. So writing a vector as a 2x2 matrix like this probably looks pretty random, but it turns out this 2x2 matrix is very useful for when we want to do geometric transformations on our vector like reflections and rotations. In order to understand why this form is useful, we need to spend a bit of time talking about the Pauli matrices, also called the Sigma matrices. Later in the video, we’ll see how these matrices can help us reflect and rotate vectors. So here are the 3 Pauli matrices, sigma X, sigma Y, and sigma Z. These are also called Sigma 1, Sigma 2, and Sigma 3 in some sources. These matrices have some nice properties that make them useful for doing geometry. 1st, they have zero trace. 2nd: they’re all Hermitian. 3rd: they all square to the identity matrix. And 4th: we can always swap the order of the sigma matrices in a product if we include a negative sign. Another way of saying this is saying that the sigma matrices anti-commute. Let’s review each of these properties in order. The trace of a matrix is just the sum of the elements along the main diagonal. It’s pretty easy to see that each sigma matrix has zero trace. Let’s move onto the Hermitian property. Remember, the transpose of a matrix, denoted with T, just means we flip a matrix’s rows and columns. But when matrices have complex entries, we’re usually more interested in the Hermitian conjugate, denoated with a dagger symbol. This means we take the transpose of the matrix AND take the complex conjugate of each entry, which flips the sign of each imaginary “I”. All 3 sigma matrices are Hermitian, which means they are equal to their own Hermitian conjugates. Both sigma X and sigma Z are real matrices, so we can just take the transpose to see this, and ignore the complex conjugate. For sigma Y, we first flip the rows and columns, which exchanges +i ad -i. Then the complex conjugate flips the + and – signs, so we end up back with where we started. So all the sigma matrices equal their own daggers. The 3rd property says that all the sigma matrices square to the identity matrix, which I’m going to denote by this fancy number 1 for this video. I’ll show this explicitly for sigma Y. The inner product of the first row and first column gives us +i * -i , and since i^2 = -1, this is just negative -1, or +1. The other inner products give us zero… zero… and another +1, which is the identity matrix. You can check that sigmaX and sigmaZ also square to the identity matrix. The final property is that, given the product of 2 DIFFERENT sigma matrices, this product equals the negative of the same 2 sigma matrices, multiplied in the same order. I’ll show this explicitly for SigmaY * sigmaZ, which gives the components zero, “i”, “i”, and zero. If we reverse the product and do SigmaZ * SigmaY, we get the components zero, “-i", “-i", and zero, which is the negative of the result we got before. You can check yourself that this property also applies to multiplying the other pairs of sigma matrices. These last 2 properties, that sigma matrices square to the identity and anti-commute, can be summarized by this formula. This delta symbol is called the Kronecker delta, which equals 1 if the I and J indices are equal, and equals zero if they are different. Basically, if we put in the same sigma matrix in for I and J, both products square to the identity, so we get 2 times the identity. But if the two sigma matrices are different, then the anti-commutative property means the terms cancel to zero. This formula can be shorted even further using this notation, where these curly brackets denote the “anti-commutator” of two matrices. Now that we’ve studied the properties of the sigma matrices, we’re going to introduce the Pauli Vector form of writing a vector. We normally write a 3D vector V with components X,Y,Z as a linear combination of basis vectors vectors eX, eY, and eZ, each scaled by the components. What we’re going to do now is replace the basis vectors with the 3 Sigma matrices, so we now have a linear combination of sigma matrices. If we write the matrices explicitly, multiply in the components, and add them together, we get this 2x2 matrix. We call this a “Pauli vector”, and it’s an alternative way of writing a 3D vector. Pauli vectors are also traceless, since the individual sigma matrices are traces. And Pauli vectors are Hermitian, since each of their individual sigma matrices are also Hermitian. You can also check for yourself that multiplying a Pauli matrix by itself will give the vector's squared length times the identity matrix. This Pauli vector might seem strange, but it’s vector useful for doing geometric operations, such as reflections and rotations. Remember, if we want to reflect our vector along the Z axis, we just leave the X and Y components alone, and flip the sign of the Z component. This basically treats the XY plane as a mirror that reflects our vector, reversing its sign along the Z direction. I’m now going to introduce an operation called conjugation. When we conjugate by sigmaZ, it means we multiply on the left by sigmaZ and on the right by sigmaZ-inverse. For example conjugating sigmaX by sigmaZ looks like this. The colours here don’t mean anything-—they’re just to make things more readable. However, since SigmaZ squares to the identity, it is its own inverse, so we can just ignore the inverse symbol here. Since we know the sigmas anti-commute, we can flip the order of SigmaX and SigmaZ here and get a negative sign, and replace this product of sigmaZs by the identity, so we get negative sigmaX. Conjugating sigmaY by sigmaZ also gives us negative sigmaY. Conjugating sigmaZ by sigmaZ just gives us positive sigmaZ, since no swapping is needed to simplify. So if we want to reflect a vector along the Z-axis, we just need to flip the sign of the Z-component. But here, we’ve done the exact opposite of that—we’ve flipped every component’s sign EXCEPT the z component. But this is easy to fix. We just put a negative sign in front of our conjugation. So NEGATIVE conjugation by sigmaZ will flip the sign of the z-component, and leave all the other components alone. Now, watch what happens when we take a general Pauli vector V and negative conjugate by SigmaZ. We can expand our Pauli vector as a linear combination of the sigma matrices, and pull all the vector components out in front, since they are just numbers. We know all these negative conjugations leave the X and Y components unchanged, and flips the sign of the Z component. So we’ve showed negative conjugating a Pauli Vector by Sigma Z, will reflect the vector along the Z axis. We can get similar formulas for doing reflections along the X and Y axes. It turns out we can reflect a vector along any direction that we like. We just pick a unit vector U of length 1, pointing in the direction of the reflection, and negative conjugate our Pauli vector V. Remember, since U is a Pauli vector of length 1, and squareing a Pauli vector gives its squared length times the identity matrix, U^2 should just equal the identity matrix. To see that the reflection actually happens, let’s negative conjugate V by a unit vector U. We can break up V into a portion that’s parallel with U, and perpendicular to U. The parallel part would just be U scaled by a factor k. And for perpendicular vectors, it turns out we can swap their order if we pick up a negative sign. We can collapse these products to identities, and re-write U*k as the parallel portion of V. So we see that the portion of V that’s parallel to U has been rerevsed, while the perpendicular part has been left unchanged. This is exactly what we expect for reflections. To prove this anti-communting property that we used for perpendicular vectors, you can pause and look and these slides. The sigma matrices are also useful for doing rotations of vectors. The key to doing rotation is understanding that rotation can be viewed as a pair of reflections. Let’s say that we have this flag, and reflect it in this mirror, so that the flag is pointing in the opposite direction. Now let’s reflect it again in this mirror. The result is just a rotated version of our original flag. So this shows us that a rotation is basically just like 2 reflections. Also notice that the angle of the flag's rotation is equal to twice the angle between the two mirrors that we used. So if the angle of rotation is theta, then then the angle between the mirors is theta/2. Let’s try reflecting our Pauli vector once along the X axis and once along the Y axis to see what we get. Geometrically, reflecting along X, and then along Y looks like this: it’s a 180-degree rotation in the XY plane. This makes sense, since the angle between our mirrors is 90 degrees, and the resulting rotation is twice that: 180 degrees. Let’s try doing this algebraically. To reflect along X, we do a negative sandwich with sigmaX. To reflect along Y, we do a negative sandwich with SigmaY. The negative signs cancel. Since we know the sigma matrices are hermitian, we can add daggers to them without changing anything. We also know from linear algebra we can replace a product of daggered matrices if we reverse their order and dagger the entire product. So basically, daggering the sigmas is the same as reversing their order. So this is the formula for doing a 180 degree rotation in the XY plane. And if we work through the algebra, we find the X and Y components get reversed, as expected for a 180-degree rotation. Now, just for curiosity’s sake, let’s see what happens if we try transforming V using a single-sided transformation, instead of a double-sided transformation. If we work through the algebra, we get that the XY components have been rotated by 90 degrees in the XY plane. However, the z component has come out wrong, with sigmas that cannot be meaningfully simplified. So, doing a double-sided transformation is required to make all the basis sigmas work out properly. Now, what if we wanted to rotate our Pauli Vectors by an arbirary angle theta? We would need 2 mirrors separated by an angle of theta/2. We could start with reflecting along the x-axis, then use another reflection vector angled at theta/2 compared to the x-axis. When we do the two reflections, the negative signs cancel. And after distributing, the sigmaX^2 become the identity matrices, and we can swap the order of these sigmas by changing the sign in front. We can write the right-side term as the hermitian conjugate of the left-side term. Since the Hermitian conjugate swaps the order of the sigmas. So this is our formula for rotating in the XY plane. You'll notice that I'm using angles of theta/2 here. That's because each side of the transforms will rotate by theta/2, giving us a rotation of theta in total. Keep in mind if the angle between the mirrors is zero, we just reflect in the same mirror twice, leading to no change, as expected. If I try to calculate this rotation, the algebra between all the sigmas gets pretty tedious. It's easier to write things out as matrices. Looking at this left expression and writing out the matrices, we get this, and we can re-write the cosines and sines using complex exponentials. The left matrix multiplies the top row of our Pauli vector by a negative exponential and the bottom row by a positive exponential. After taking the hermitian conjugate of the right matrix, we get similar multiplications for the left column and right column of our Pauli vector. The exponentials next to the Zs cancel, and we end up with a complex exponential multiplying our x and y components by a phase of theeta. Taking this entry and multiplying things out, the real part is our new x-component, and the imaginary part is our new y component. These transformations match exactly what we'd get from a rotation matrix in 3 dimensions by an angle theta in the XY plane, so our 2-sided rotation of our Pauli vector worked. We can get similar matrices for rotations in the YZ plane and ZX plane. It turns out that these are all Special Unitary 2x2 matrices, or SU(2) matrices, meaning they are unitary and have determinants of 1. As an exercsise, you can try proving all these matrices are unitary, by multiplying them by their hermitian conjugate and getting the identity matrix. See if you can do it by using only the properties of the sigma matrices, without writing the matrices explicitly. So Pauli vectors are rotated with double-sided transformations of SU(2) matrices. Let’s see if we can actually prove this fact definitively. As a starting point, we’re going to assume that rotating a Pauli-vector can be done with a double-sided transformation with matrices A and B. We’re going to use the properties of Pauli vectors to deduce what A and B should look like. We already know what Pauli vectors are Hermitian and have zero trace. But another important property of rotating a 3D vector is that its length will not change after the rotation. So we expect the squared length: x^2 + y^2 + z^2 to be unchanged after rotation. Notice that when we take the determinant of our Pauli vector, we get z*(-z) MINUS this times this… and simplifying this becomes -x^2 - y^2. So the determinant of our Pauli vector is the NEGATIVE of the vector’s squared length. So when we rotate a Pauli vector, its determinant, which is the vector's negative squared length, should not change. Using these 3 properties, we can deduce the form of the A and B matrices. Since Pauli Vectors are Hermitian, the resulting Pauli Vector after the transformation should also be Hermitian. And remember, the Hermitian conjugate of a product is the product of the Hermitian conjugates, in the reverse order. Since V equals V-dagger, we get this formula. We can use this as a hint to assume that the B matrix should equal A-dagger. So our transformation looks like AVA-dagger. Next, we know that the determinant, or negative squared length, of our Pauli vector should NOT change after the transformation, so these two determinants must be equal. If we recall that the determinant of a product is the product of determinants, we can cancel the determinant of V on both sides. The transpose does not change the determinant result, so we ignore it, and we can bring this complex conjugate outside. So we end up with the squared magnitude of the determinant of A being 1. This means the determinant of A can be any complex number with magnitude 1… meaning it’s a complex number on the unit circle of the form e^i*theta. However, there’s an important detail we need to pay attention to with double-sided transformation. Let’s try replacing our matrix "A" with "A times a complex phase". The phase on the right will get complex conjugated because of the Hermitian conjugate, causing it to cancel with the phase on the left. The resulting transformation is the exact same as the one before we multiplied by the phase. This means there are multiple A matrices that lead to the same transformation, all related by different complex phase factors. So there is a redundancy in the A matrices for doing rotations. We can pick out a specific A matrix by picking the one whose determinant is equal to 1. If A has a determinant of e^i*theta, we can always force its determinant to be 1 by multiplying it by a phase of e^-i*theta/2. This phase will be ignored as a result of the double-sided transformation, so the phase drops out anyway. But we can successfully force the determinant of A to be 1. So, by using the properties of determinants and eliminating the redundancies in the phase, we know A has a determinant of +1. Finally, since Pauli Vectors have zero trace, the result of the transformation should also have zero trace. When we have the trace of a product of matrices, a theorem from linear algebra tells us we can cycle the product order, bringing the last one and putting it in front. Now, since the trace of V is zero for any possible V, the only way for this trace to equal zero is if A-dagger A equals the identity matrix times a constant. This can be shown by checking various cases of V and solving for the matrix entries of A-dagger*A. Taking the determinant of this formula, we already know from the last step that the determinant of A is 1, so k equals + or - 1. So A-dagger times A equals + or – the identity matrix. However, the product A-dagger * A is guaranteed to have positive entries along the diagonal, so the only option is that A-dagger * A equals the identity matrix. In other words. A-dagger equals A-inverse. The fact that A-dagger equals A-inverse means that A is member of the group of 2x2 UNITARY matrices, U(2). And the fact that it has a determinant of +1 means it is from the group of 2x2 SPECIAL UNITARY matrices, SU(2), where the word “special” means the determinant is 1. So what we’ve shown is, in order to rotate a Pauli vector by some angle, we do a double sided matrix transformation like this, with SU(2) matrices. This is a direct result of Pauli vectors being traceless, Hermitian, and having a determinant that's equal to the negative squared length of a vector, which must be that is unchanged by rotations. Now, what do SU(2) matrices actually look like? If we take an SU(2) matrix A with complex entries alpha, beta, gamma, delta, we know that A-dagger involves flipping the off-diagonal components and complex conjugating all the entries. We also know that the inverse of any general 2x2 matrix will take this form. Since the determinant of an SU(2) matrix is 1, we can set this to 1. And because A-dagger equals A-inverse, and we immediately get that delta = the complex conjugate of alpha, and gamma = the negative complex conjugate of beta. So all SU(2) matrices take this form, where the determinant equals 1. So the squared magnitude of alpha + the squared magnitude of beta = 1. The 3 rotation matrices we saw earlier match this form, so they are indeed SU(2) matrices. Also note that SU(2) matrices have 2 complex parameters, or 4 real parameters… however, since the determinant is forced to be 1, this gives one extra constraint, reducing the 4 real parameters to 3 real parameters, as there are 3 planes of rotation we can choose rotation angles for. Another thing to notice is that the positive version of an SU(2) matrix, and the negative version of an SU(2) matrix perform the exact same rotation, since the negative signs cancel out in the double-sided transformation. This means that for every 3x3 rotation matrix in the group SO(3), there there are 2 corresponding rotation matrices in SU(2) that do the same rotation. We’ll see later this means SU(2) is the “double cover” of SO(3). So to summarize this video, we saw that the sigma matrices, also called Pauli matrices, obey these special anti-commutation relations. And this allows them to easily perform geometric transformations on Pauli vectors. Reflections are performed by doing negative conjugation on a pauli vector, and Rotations are performed using double-sided transformations like this, where these matrices correspond to SU(2) matrices. Each of the 2 matrices performs half the rotation, so we use half-angles inside the matrix entries. In the next video, we’ll see how we can factor a Pauli vector into a pair of Pauli spinors. Each Pauli spinor will rotate with only a single SU(2) matrix. This is why Pauli spinors rotate half as much as vectors do, and require two full rotations to get back to their starting position.