We're continuing to talk about representations of Lie groups and Lie algebras. In this video we're going to discuss the tensor product of representations. In physics courses, the tensor product of representations gives us instructions for addition of angular momentum in systems of multiple particles. In previous videos, I mentioned how all fundamental particles have a property called "spin", which is either an integer or a half-integer. Spin-zero particles transform like scalars. Spin-1/2 particles transform like spinors. Spin-1 particles transform like vectors. And there are higher-spin particles as well. An important fact here is that all these objects can be constructed from tensor products of different numbers of spinors. So in a certain sense, spinors are the most basic fundamental object that can be used to build all other particle-like objects. So in this video we're going to look at systems of multiple spin-1/2 particles, or spinors, and see how they combine together into multi-particle systems using the tensor product. As an example, a pair of spin-1/2 electrons can bind together in a solid at low temperatures into Cooper pairs (or BCS Pairs) and behave more like a spin-1 vector particle or a spin-0 scalar particle. This is a key fact underlying the phenomenon of super-conductivity, because spin-1 and spin-0 particles are bosons and bosons can all occupy the same state, unlike half-integer-spin fermions which must obey Pauli's exclusion principle. In the language of representation theory, we would say that the tensor product of two irreducible spin-1/2 representations becomes the direct sum of a spin-1 irrep and a spin-0 irrep. Rephrased using dimensions, the tensor product of a pair of 2x2 irreps equals the direct sum of the 3x3 irrep and the 1x1 irrep. This will lead us to the Clebsh-Gordan coefficients, which tell us how to take spinors in the coupled basis and build states in the total angular momentum basis. Also, in this video I will often refer to all kets as "states" or "vectors" because they live in vector spaces. But when I talk about the "vector representation" of su(2) I'm specifically talking about the spin-1 representation of su(2) which transforms 3D vectors. So before we discuss tensor product representations, let's review the direct sum and the tensor product. The basic summary is that the direct sum will add dimensions of two vector spaces, and the tensor product will multiply the dimensions of two vector spaces. If we have a vector v in 3D space and a vector w in 2D space. we can write both of them as columns. The direct sum of v and w involves stacking the columns on top of each other into a 5D vector. This vector lives in a new 5-dimensional space with 5 basis vectors. which is just the 3D basis and the 2D basis put together. If there's a 3x3 linear map L that act on v, and a 2x2 linear map K that acts on w, then the direct sum of L and K is a 5x5 block diagonal matrix like this. If we multiply this onto the direct sum of v and w, the L part of the matrix acts on v and the K part of the matrix acts on w. More abstractly, if we see the direct sum of L and K act on the direct sum of v and w, we can just pass v to L and pass w to K to get our result. The tensor product, on the other hand, is a bit harder to understand. The tensor product of a vector v in 3D space and a vector w in 2D space is a 6-dimensional vector. The new basis in six-dimensional space uses all possible pairs of basis vectors from from the 3D space and the 2D space. One way to think about the tensor product of v and w is as the column row outer product of v and w-transpose, which gives us 6 components. In this case, we're viewing v as a vector and viewing w-transpose as a dual vector. To see how this 6-component object transforms, we can transform v using the 3x3 L matrix from the left, and transform w transpose using the 2x2 K transpose matrix from the right. So we get a double-sided transformation like this. If we're dealing with complex vector spaces, we use the Hermitian conjugate operation instead of the transpose operation. However, it's also good to know how to take the tensor product of two column vectors. The tensor product of arrays is also called the "Kronecker product", and it works as follows: the tensor product of the column v and the column w involves taking each component of v and multiplying it by an entire copy of the two-component column w. The result is a 6-component column with the components of v and w mixed together. Similarly, for the tensor product of matrices L and K, we can multiply each component of L in the 3x3 matrix by an entire copy of the 2x2 matrix K. The result is a 6x6 matrix that can operate on our 6-component column. The resulting component multiplications end up being the same as the column-row version we showed earlier. More abstractly, if we see the tensor product of L and K act on the tensor product of v and w, we can pass v to L and pass w to K to get our result. An important difference between the direct sum and the tensor product is how scalar multiplication works. When we scale the direct sum of vectors v and w, we're scaling the entire column, so both v and w get scaled by the scalar. So when we scale a direct sum, both elements in the sum get the scaling coefficient. However, since the tensor product of vectors v and w is done as a kind of array multiplication, we scale the mixture of v and w. So when we scale this tensor product, we can choose to either scale v or scale w, but we don't scale both. Also, the tensor product obeys the distributive property over vector addition in both directions. When we deal with systems of multiple particles, the laws of quantum mechanics say that we must combine the particle states together using the tensor product. For example,if we have a system of two spin-1/2 particles, each with two states, then the tensor product space has a basis given by all possible pairs of particle states. We can rotate these state pairs using the tensor product of a pair of SU(2) matrices. For a system of three spin-1/2 particles, the tensor product space has a basis given by all possible triples of particle states. We can then rotate them using the tensor product of a triple of SU(2) matrices. To save space, in quantum mechanics, we often omit the tensor product symbol and write all the particle states in a single ket like this. So whenever we have matrices from a Lie group like SU(2), we know the rule for how the tensor product of Lie group matrices will act on the tensor product of states. But what about the matrices in a Lie group's Lie algebra? For example, how would the ladder operators and eigenvalue operator work on tensor product states? Recall, given a Lie group with the identity element as one of its points, the group's Lie algebra is like a tangent space at the identity of the group. A Lie algebra element is like a tangent vector in this tangent space. When a Lie algebra element is exponentiated along with a parameter, it will generate a continuous path of group members in the Lie group. When the parameter is zero, we get the identity element, which is where the Lie algebra tangent space meets the Lie group. To go from the Lie group matrix back to the generator in the Lie algebra, we take the derivative of the group element and then set the parameter to zero. This derivative brings the Lie algebra generator down in front by Chain Rule, and setting the parameter to zero makes the exponential go to the identity, leaving us with just the generator. Let's say that we have a Lie group path A(t) with generator small a and another path B(t) with generator small b. We can get these generators by taking the derivative and then setting the parameter t to zero. If we have the tensor product of A(t) and B(t), we can find this element's generator the usual way by taking the derivative and setting the parameter t to zero. To take the derivative, we use product rule over the tensor product and get a sum of two terms. Then we set the parameter t to zero. By definition, A(0) and B(0) both equal the identity element, since the parameter being zero gives the identity transformation. And setting t equals zerp for the derivatives is just the definition of the original generators small a and small b. So we see the tensor product group element is generated by a⊗1+1⊗b. So we can obtain this Lie group member A(t)⊗B(t) by exponentiating this Lie algebra generator times a parameter t. So let's say we have two Lie group representations rho1 and rho2 that give us matrix representations for elements of a group G. We can define a new group representation rho1⊗rho2 of a group element just by taking the element's first representation matrix and second representation matrix and taking the tensor product of the matrices. Therefore, the tensor product of a 3x3 representation and a 2x2 representation would give us a 6x6 representation. But what about the corresponding Lie algebra representation? If we have two representations pi1 and pi2 of a Lie algebra small g, then the tensor product representation of an algebra element is given by the first representation matrix tensored with the identity plus the identity tensored with the second representation matrix. If small l generates L(t), 3X3 matrix, and small k generates K(t), a 2x2 matrix, then L(t)⊗K(t) will be generated by small l tensor the 2x2 identity, plus the 3X3 identity tensor small k. Note that after we carry out the tensor product, both matrices in the sum will be 6x6. So we get a sum of two 6x6 matrices. So these definitions work for any pair of Lie group representations and their Lie algebras, but let's get back to the specific case of the SU(2) Lie group and the su(2) Lie algebra. Let's see what happens when we apply these rules to the case of a pair of spin-1/2 su(2) representations. Recall, the su(2) Lie algebra is defined using these commutation relations of three generators. And the raising and lowering operators and eigenvalue operator are defined like this. For the spin-1/2 representation of su(2), the generators are given by 2x2 matrices. These generators act on a two-dimensional vector space with two basis elements. It's common to label these basis elements as +1/2 and -1/2, which are their eigenvalues under the gz operator. However, for simplicity, I'm going to label them with an up arrow and a down arrow, respectively, because they denote the spin-up and and spin-down states of a particle. If we have a pair of su(2) Lie algebras, each represented with the spin-1/2 representation, and we take their tensor product, the dimensions will multiply into a four-dimensional space. The basis states would then be up-tensor-up, up-tensor-down, down-tensor-up, and down-tensor-down. Again, to save space, I'll often omit the tensor product symbol and write the arrows in the same vector. But I'll be using both of these notations in this video. We can take the generators in the su(2) Lie algebra and extend them to the space su(2)⊗su(2). We just take the operator tensor the identity, plus the identity tensor the operator. The eigenvalue and ladder operators, which live in the complexified version of these Lie algebras, can be extended in the same way. Do the notation here can get a little confusing, but the eigenvalue operator that lives in the su(2) tensor product space is made up of eigenvalue operators from the individual su(2) spaces. I'm going to use the same symbol for both of these cases but it should be clear from Context what I mean. If an operator acts on an individual state, it's an individual su(2) operator. If an operator acts on a tensor product state, it's the tensor product su(2) operator. So we can use these tensor product ladder operators to travel up and down the ladder of states in the tensor product space, and use the tensor product eigenvalue operator to get the eigenvalues of the tensor product states. Let's start by getting the eigenvalues for these four states under the gz operator, starting with the up-up state. We can write out the operator in the tensor product representation using the definition we just learned for Lie algebras. And we can write the state using the tensor product notation. Here we have a sum of two linear maps, so we just distribute the input to each linear map in the sum. Next, we pass the tensor product inputs to the tensor product operators: the first state goes to the first operator and the second state goes to the second operator. The gz's acting on the up states will return an eigenvalue of +1/2, and the identity operators just leave the states as they are. Using the rules of the tensor product, we can pull the 1/2s out in front, and then add the states together. We find that the eigenvalue of the up-up state is 1. Now, notice what happened here is that the first gz operator acted on the first state and the second gz operator acted on the second state. And then we just added the two eigenvalues together to get 1. So if we have a state with eigenvalue m1 and another state with eigenvalue m2, and we take their tensor product... in the tensor product states, their combined eigenvalue is just "m1 + m2". This is because the first state gets a gz and the second state also gets a gz, so the eigenvalues are just summed together. So let's use this trick to quickly get the eigenvalues of our tensor product states. We already know that the eigenvalue of the up-up state gives 1/2 + 1/2 = 1. The up-down state will have an eigenvalue of a 1/2 - 1/2 = 0. The down-up state will have an eigenvalue of -1/2 + 1/2 = 0. And the down-down state will have an eigenvalue of -1/2 -1/2 = -1. If this isn't obvious, I'd encourage you to pause the video and try the calculations manually yourself. Now, let's try using the raising and lowering operators on these states. Recall, the raising operator sends the down state to the up state and sends the up state to zero. The lowering operator sends the up state to the down state and sends the down state to zero. Basically, we can't raise a state of above the highest eigenvalue and we can't lower a state below the lowest eigenvalue. And recall, in this case the eigenvalues are sometimes called "weights". In our tensor product space, the up-up state has the highest eigenvalue, so it would make sense that the raising operator would send it to zero. Let's confirm that. We can write the operator and state out fully with tensor products, then we can pass the input state to the two linear maps, then pass the first states to the first operators and the second states to the second operators. Now, here, when the raising operators acts on the up states, they go to zero. So both tensor products go to zero and we get zero as a result. So the up-up state is indeed the highest weight state, as expected. Now, we can use the lowering operators to lower this state down the ladder and get a basis of eigenstates under gz. If we apply the lowering operator to the up-up state, we get a sum of two terms where the first term has the first state lowered to down and the second term has the second state lower to down. So we end up with the state down-up plus up-down. If we apply the eigenvalue operator to this state, we get zero. So we have indeed lowered the eigenvalue by 1. If we now apply the lowering operator again, we have two inputs sent to a sum of linear maps. So expanding, we get four terms. Lowering on the down states gives zero and lowering on the up states gives down states. So we end up with the down-down state with a coefficient of 2 in front. The eigenvalue operator gives an eigenvalue of -1. So again, we lowered the eigenvalue by 1.Now, one thing to remember is that the raising and lowering operators give result states that are multiplied by coefficients which I called alpha and beta in the previous video. These coefficients depend on the spin representation number j and the state eigenvalue m. Since the eigenvalues of these states are +1, 0, and -1, this is a spin-1 representation, and the beta coefficients are the square root of 2. If we take the states obtained from the lowering operator and take these coefficients into account, we get this set of three states for our spin-1 representation. These states are actually normalized to 1, so they're easier to work with. Now, a problem here is that if we try to lower the down-down state, we get zero. So this is the lowest-eigenvalue state. This is a bit weird because the tensor product space is 4-dimensional but the space given to us by the latter operators is only 3-dimensional. So what happens to the extra dimension? It turns out the answer to this mystery is that the space actually has TWO highest-weight vectors. The one we missed is down-up minus up-down. If we apply the raising operator to it, we get up-up minus up-up which is zero. So this is indeed a highest-weight state. And if we lower the state, we get down-down minus down-down, which is zero. So this is also a lowest-weight state. The state also has an eigenvalue of zero. So what's going on here is that this tensor product space actually has TWO su(2) representations living inside it. We have a 3D representation with state eigenvalues +1, 0, -1. This is a spin-1 representation. These are sometimes called "triplet states". We also have a 1-dimensional representation whose state has eigenvalue zero. This is a spin-0 representation. This is sometimes called the "singlet state". So the tensor product of a pair of 2D representations of su(2) decomposes into the direct sum of a 3D representation of su(2) and a 1D representation of su(2). We often write this fact as 2⊗2 = 3⊕1. And remember, we can get the spin number j of a representation by taking the dimension n of the representation subtracting one and dividing by two. So we can also write this equation using spin numbers instead. So the tensor product of a pair of spin-1/2 representations equals the direct sum of a spin-1 representation and a spin-0 representation. However, I prefer the first formula with the dimensions because if we interpret this as an elementary school math equation, the numbers actually work out. It's also common to write these irrep basis vectors using the |j,m> notation, where j is the spin number and m is the eigenvalue. The triplet states belong to a spin-1 representation so they all have j values of 1, and their eigenvalues m are +1, 0, and -1. The singlet state belongs to a spin-0 representation so, j=0 and its eigenvalue m is also equal to zero. This procedure of taking the tensor product of two representations is what physicists call "addition of angular momentum" for two particles. The jvalue gives the total angular momentum for a system of particles, and the eigenvalue m indicates the component of angular momentum along the z-axis.In physics, these are both multiplied by h-bar. We usually use an operator associated with the square of the total angular momentum in quantum mechanics. This operator is also called the "Casimir Operator" by mathematicians. And in my videos I've been denoting it as g^2. This operator commutes with all of the elements in the Lie algebra, and when it acts on the state it returns j(j+1), where j is the spin number for the representation that state belongs to. Now, for tensor product representations, you might be tempted to try and write g^2 using the same formula we used for the other operators, but we cannot actually do this. The reason for this is that the Casmir operator doesn't actually live inside the Lie algebra, because it is defined using the squares of operators and Lie algebras don't understand what squaring operators means. Lie algebras only understand the Lie bracket. To get the correct definition of the Casimir operator in tensor product spaces, we need to take our original formula for the Casimir operator from the last video and replace each operator with the tensor product version. We get the tensor-product-style formula we're used to with Lie algebras, plus some extra parts. We can label our first particle with j1 and m1 and label our second particle with j2 and m2. Then we can combine them with the tensor product and see what we get when we apply the Casimir operator to it. If we try applying our Casimir operator to the up-up state and sub in all the related values for the first and second particles, we get a result of 2. This is exactly the result we expect for a total angular momentum of capital J equaling 1. We get the same J=1 result for all of the triplet states. And for the singlet state the Casimir operator gives us a J value of zero, so this really is the direct sum of a spin-1 and spin-0 representation, as expected. So in our tensor product space we've discovered two different bases. We have the standard tensor product basis, also called the "coupled basis". But we also have the basis where we've decomposed the space into a spin-1 irrep and a spin-0 irrep. I'll call this the "irrep basis". physicists will usually call it the "total angular momentum basis". The first number here capital J is the spin value of the representation, and the second number capital M is the eigenvalue of a state under the gz operator. So the irrep basis states can all be written out as linear combinations of the tensor product basis states. If we include the terms with zero coefficients, we get 16 coefficients in total. We can arrange all of these coefficients into a matrix with the tensor product states arranged on the side and the irrep basis states arranged up top. A given column tells us which coefficients are needed to build up that column's irrep basis state. Now, I'm going to rewrite the up states using the +1/2 eigenvalue and rewrite the down states using the -1/2 eigenvalue. I'm also going to rewrite the irrep basis states so that the spin value J is on top and the eigenvalue M is on the bottom. Next I'm going to remove the zero values in the matrix and position the basis vectors like this. This is called the Clebsch-Gordan table for the spin-1/2 ⊗ spin-1/2 representation of su(2). The way this table works is: to build a given irrep basis vector on top, you just read the column below it for the coefficients you need to build it with the tensor product basis states shown on the left. So for example, to build the "J=1, M=0", state we need 1/sqrt(2) times the "+1/2 -1/2" state, plus 1/sqrt(2) times the "-1/2 +1/2" state. You might find this table a little easier to read when it's in the full 4x4 matrix form, but with a little practice you should be able to get used to reading it like this. It's also common to remove the square roots from the coefficients to save space, so you'll need to remember to add the square roots back in yourself if the table is written like this. We can also interpret these coefficients as being the inner product of the states in the two bases. These inner product coefficients end up being the same as the coefficients in the linear combinations because all the states are normalized and orthogonal. I've included a link in the description to a page with this and other Clebsch-Gordan tables. So at this point we basically know everything we need to know about the spin-1/2 ⊗ spin-1/2 representation. We know it decomposes into a direct sum of a spin-1 irrep and a spin-0 irrep. And we know the coefficients for changing to the irrep basis. Now let's try looking at the tensor product of three spin-1/2 su(2) irreps, which has eight basis states. We get operators in the Lie algebra exactly as we did before: if we have Lie group members A(t), B(t), and C(t) with generators small a, small b, small c. We can get the generator of A(t) tensor B(t) tensor C(t) by taking the derivative and setting the parameter to zero. Product rule gives us a sum of three terms this time. And setting t=0, the three derivative terms each give a generator and the other terms just give identities. So we have a sum of three terms, each containing one Lie algebra generator in one of the three positions. This immediately gives us definitions for the eigenvalue and ladder operators. As before, the eigenvalue operator acting on a tensor product state simply gives the eigenvalues of each state in the tensor product and sums them together. So the eigenvalue of the up-up-up state is 1/2 + 1/2 + 1/2 = 3/2. This is a highest-weight state, and we can use the lowering operator to build the rest of the basis for a spin-3/2 irrep. We can also find two other highest-weight states, and we find they each give us a spin-1/2 irrep. If we take the beta coefficients from the lowering operator into account, we get these normalized states here. So the tensor product of three spin-1/2 irreps decomposes into the direct sum of a spin 3/2 irrep, a spin-1/2 irrep, and another spin-1/2 irrep. In other words, the tensor product of three 2x2 irreps decomposes into the direct sum of a 4x4 irrep and a pair of 2x2 irreps. We can see all the a basis vectors here. For spin 3/2, we get four basis vectors. And for each of the spin-1/2s, we get two basis vectors. We can go through a similar process for the tensor product of four spin-1/2s. All of the four-tensor operators work as you'd expect. I'm not going to go through the details, but this representation decomposes into the direct sum of irreps with spins 2, 1, 0, 1, 1, and 0. In terms of the dimensions, the irreps in the direct sum have dimensions 5, 3, 1, 3, 3, and 1. I'm showing the first three irreps here... and the second three irreps here. Now, there's a trick from Mark Thomson's Modern Particle Physics book that helps us get these irrep decompositions more easily. We can illustrate the 2-dimensional su(2) irrep as 2 states, represented as 2 nodes connected by a line segment. The ladder operators move use between the 2 nodes. This is called a "weight diagram".If we have two of these irreps and we take their tensor product, we can illustrate the result by taking the 1st diagram and placing a copy of the 2nd diagram centered on each of the 2 nodes. We get a weight diagram with 3 nodes. This shows that the result contains a 3D irrep, which are the triplet states. However, since the middle location here contains 2 overlapping nodes, one of them pops out and becomes a 1D irrep: the single state. This illustrates the irrep formula we saw earlier: where the tensor product of a pair of 2D irreps equals the direct sum of a 3D irrep and a 1D irrep. We can continue this pattern for the tensor product of three 2D irreps. We know the first two diagrams in the product result in the sum of a 3D irrep and a 1D irrep. Now we can distribute the product over the sum as if this were an algebra equation. To calculate these products, we can again use the trick of taking the first diagram in the product and placing a copy of the second diagram in the product over each node. The 1st term in the sum gives us a 4D irrep. However, since there are 2 node overlaps, a 2D irrep also pops out. And the 2nd term gives us a 2D irrep. In this way, we see the product of three 2D irreps gives us the sum of a 4D irrep and a pair of 2D irreps, as we saw before. We can keep going with the tensor product of four 2D irreps. We know the first three become 4 sum 2 sum 2. Then we distribute to get a sum of 3 products. Then we we use the trick of taking the 1st diagram and placing copies of the 2nd diagram over its nodes. In this way, the tensor product of four 2D irreps gives us the sum of irreps with dimensions 5,3,3,1,3,1... as we saw earlier. Now, if we look at all the tensor product representations so far, the largest irrep in each one increases by half an integer spin each time. In other words, the largest irrep increases by one dimension each time. Let's take a closer look at these largest irreps that result from the tensor products. The tensor product of two spinors gives us a largest irrep of a spin-1 particle, or vector, which is a rank-1 tensor. The tensor product of three spinors gives us a largest irrep of a spin-3/2 particle. The tensor product of four spinors gives us a largest irrep of spin-2 particle, or rank-2 tensor, and so on. In this way, the tensor product of an even number of spinors can be thought of as the fundamental building block for all tensor ranks. And the tensor product of an odd number of spinors gives a tensor that can be thought of as having a half-integer rank. These are sometimes called spinor-tensors. The largest irreps inside the tensor product spaces all have the property of being totally symmetric. This means that if you swap any two spin values in the basis states, the overall state is unchanged. For example, in this spin-3/2 irrep, if I exchange the positions of the first and second spin states in all the basis states, the overall basis states are unchanged. So all the tensors and spinor-tensors associated with su(2) in three dimensions of space can be constructed as totally symmetric states of some number of spinors multiplied together with the tensor product. Now, so far in this video we've been focusing on the tensor product of spin-1/2 su(2) representations. But we can also take the tensor products of representations of other sizes as well. For example, the tensor product of a spin-1/2 irrep and a spin-3/2 irrep, or the tensor product of a spin-1 irrep and a spin-1 irrep. In each case, we can change the basis from the tensor product basis to the irrep basis, which is where the space is decomposed into a direct sum of smaller irrep. The change-of-basis equations for doing this are given by sets of larger Clebsch-Gordan tables, which I've linked in the description. To use the tables, you start by identifying the two representations you're tensoring together. This tells you which table to use. In this example, we'll look at spin-1 tensored with spin-1. Along the top, you'll see the various basis vectors in the irrep basis labeled with their spin number J followed by their eigenvalue M. For example, here we have a decomposition into a spin-2 irrep with 5 basis states, a spin-1 irrep with 3 basis states, and a spin-0 irrep with 1 basis state. Along the sides, you can see pairs of eigenvalues m1 and m2 for the two vectors being used in the tensor product. To get the coefficients for a given irrep vector, we just read the column for the coefficients and looked to the left for the basis states that they are matched with. Don't forget that you need to include the square root signs over these coefficients because they have been left out to save space. The minus signs always belong on the outside of the square root signs. I've linked to some other videos in the description if you want more details on calculating and reading these tables. The trick with drawing weight diagrams also works here. To compute the product of a pair of 3D irreps, we take the first diagram, and place a copy of the 2nd diagram centered on each node. We see we get a 5D irrep. 3 of the nodes have 2 overlaps, so a 3D irrep pops out. Also the middle node has 3 overlaps, so the final one popsout as a 1D irrep. As expected, the tensor product of a pair of 3D reps decomposes into 5 sum 3 sum 1. So this concludes the explanation of tensor product representations of su(2). I still need to talk about how the tensor product is involved with representations of sl(2,C) for space time. However, I've realized that topic is so big that it requires its own video so that will be coming next next.